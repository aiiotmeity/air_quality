# -*- coding: utf-8 -*-
"""finaltestkriging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BIpcSkptoq6heP0Ld0saSTkLJUd7GiD6
"""


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from django.contrib.admin import display
from sklearn.cluster import KMeans
from sklearn.datasets import data
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error
from pykrige.ok import OrdinaryKriging
import geopandas as gpd
import folium
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Function to perform Kriging interpolation for a pollutant
def perform_kriging(data, pollutant):
    # Define the grid for interpolation
    grid_lon = np.linspace(data['Longitude'].min(), data['Longitude'].max(), 50)
    grid_lat = np.linspace(data['Latitude'].min(), data['Latitude'].max(), 50)
    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)

    # Perform Ordinary Kriging
    OK = OrdinaryKriging(data['Longitude'], data['Latitude'], data[pollutant],
                         variogram_model='linear',
                         verbose=False,
                         enable_plotting=False)
    z, ss = OK.execute('grid', grid_lon, grid_lat)

    # Convert z to regular numpy array (if it's a MaskedArray)
    z = np.array(z)

    return grid_lon, grid_lat, z

# Function to create an interactive map showing the interpolated results
def create_interactive_map(data, pollutant):
    grid_lon, grid_lat, z = perform_kriging(data, pollutant)

    # Initialize the map centered around the mean of coordinates
    center_lat = np.mean(data['Latitude'])
    center_lon = np.mean(data['Longitude'])
    map_osm = folium.Map(location=[center_lat, center_lon], zoom_start=10)

    # Add the interpolated values as a heatmap layer
    heatmap_layer = folium.raster_layers.ImageOverlay(
        z,
        bounds=[[grid_lat.min(), grid_lon.min()], [grid_lat.max(), grid_lon.max()]],
        opacity=0.6,
        origin='lower',
        colormap=lambda x: (0, 0, x, x),
        name=f'{pollutant} concentration'
    ).add_to(map_osm)

    # Add interpolated value markers with tooltips
    for lat, lon, value in zip(grid_lat.flatten(), grid_lon.flatten(), z.flatten()):
        folium.CircleMarker(
            [lat, lon],
            radius=5,
            popup=f"{pollutant}: {value:.2f}",
            color='white',  # Adjust color as needed
            fill=True,
            fill_color='white',  # Adjust color as needed
            fill_opacity=0.2,
            tooltip=f"Interpolated {pollutant}: {value:.2f}"
        ).add_to(map_osm)

    # Add the sensor locations as markers
    for idx, row in data.iterrows():
        folium.Marker(
            [row['Latitude'], row['Longitude']],
            popup=f"Sensor {pollutant}: {row[pollutant]}",
            icon=folium.Icon(color='red', icon='info-sign')  # Adjust color and icon as needed
        ).add_to(map_osm)

    # Add layer control to toggle heatmap and markers
    folium.LayerControl().add_to(map_osm)

    # Display map in Jupyter notebook
    display(map_osm)

# Step 1: Data Collection and Preprocessing
# Load sensor data
sensor_data = pd.read_csv("sensor_data.csv")

# Load weather data
weather_data = pd.read_csv("weather_data.csv")

# Load geospatial data
geospatial_data = pd.read_csv("geospatial_data.csv")

# Load additional data
additional_data = pd.read_csv("additional_data.csv")


# Merge all data
merged_data = pd.merge(sensor_data, weather_data, on=['Sensor_ID', 'Timestamp'])
merged_data = pd.merge(merged_data, geospatial_data, on='Sensor_ID')
merged_data = pd.merge(merged_data, additional_data, on='Sensor_ID')

# Ensure the columns are correctly loaded
print(merged_data.columns)

# Initialize LabelEncoder for Wind Direction
label_encoder = LabelEncoder()
# merged_data['Wind_Direction'] = label_encoder.fit_transform(merged_data['Wind_Direction'])

# Convert categorical wind directions to numerical values
weather_data['Wind_Direction'] = label_encoder.fit_transform(weather_data['Wind_Direction'])

# Display the first few rows of merged data
print("Merged Data:")
print(merged_data.head())

# Summary statistics
print("\nSummary statistics:")
print(merged_data.describe())

# Check for missing values
print("\nMissing values:")
print(merged_data.isnull().sum())

# Data visualization
# Create subplots for histograms and box plots of all pollutants
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Histograms
sns.histplot(merged_data['NO2_Level'], bins=30, kde=True, color='blue', ax=axes[0, 0])
axes[0, 0].set_title('Histogram of NO2 Levels')

sns.histplot(merged_data['PM25_Level'], bins=30, kde=True, color='green', ax=axes[0, 1])
axes[0, 1].set_title('Histogram of PM2.5 Levels')

sns.histplot(merged_data['PM10_Level'], bins=30, kde=True, color='red', ax=axes[0, 2])
axes[0, 2].set_title('Histogram of PM10 Levels')

sns.histplot(merged_data['O3_Level'], bins=30, kde=True, color='orange', ax=axes[1, 0])
axes[1, 0].set_title('Histogram of O3 Levels')

sns.histplot(merged_data['NH3_Level'], bins=30, kde=True, color='purple', ax=axes[1, 1])
axes[1, 1].set_title('Histogram of NH3 Levels')

sns.histplot(merged_data['SO2_Level'], bins=30, kde=True, color='brown', ax=axes[1, 2])
axes[1, 2].set_title('Histogram of SO2 Levels')

plt.tight_layout()
plt.show()

# Box plots
plt.figure(figsize=(18, 6))

sns.boxplot(y='NO2_Level', data=merged_data, color='blue', width=0.2)
sns.boxplot(y='PM25_Level', data=merged_data, color='green', width=0.2)
sns.boxplot(y='PM10_Level', data=merged_data, color='red', width=0.2)
sns.boxplot(y='O3_Level', data=merged_data, color='orange', width=0.2)
sns.boxplot(y='NH3_Level', data=merged_data, color='purple', width=0.2)
sns.boxplot(y='SO2_Level', data=merged_data, color='brown', width=0.2)

plt.title('Box Plots of Pollutant Levels')
plt.xlabel('Pollutants')
plt.ylabel('Pollutant Levels')
plt.xticks(rotation=45)
plt.legend(['NO2', 'PM2.5', 'PM10', 'O3', 'NH3', 'SO2'])
plt.show()

# GeoVisualization
gdf = gpd.GeoDataFrame(sensor_data, geometry=gpd.points_from_xy(sensor_data.Longitude, sensor_data.Latitude))
m = folium.Map(location=[40.7128, -74.0060], zoom_start=12)
for _, row in gdf.iterrows():
    folium.CircleMarker([row['Latitude'], row['Longitude']], radius=5, color='blue', fill=True).add_to(m)
m.save('sensor_map.html')

# Select features for clustering
merged_data = pd.merge(sensor_data, weather_data, on=['Timestamp', 'Sensor_ID'])
features = merged_data[['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level', 'CO_Level', 'Temperature', 'Humidity', 'Pressure', 'Wind_Speed','Wind_Direction']]

# Verify if the features DataFrame is not empty
print(features.shape)  # Check the number of rows and columns
print(features.head()) # Inspect the first few rows of the DataFrame

# Apply k-means clustering
kmeans = KMeans(n_clusters=3)
merged_data['Cluster'] = kmeans.fit_predict(features)

# Plot clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Longitude', y='Latitude', hue='Cluster', data=merged_data)
plt.title('Clusters of Pollution Levels')
plt.show()

# Assuming merged_data contains pollutant levels and geographical coordinates

# List of pollutants to include
pollutants = ['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level']

# Initialize empty DataFrame to store cluster labels
clustered_data = pd.DataFrame(index=merged_data.index)

# Perform clustering for each pollutant
for pollutant in pollutants:
    # Select features (pollutant level and geographical coordinates)
    features = merged_data[[pollutant, 'Latitude', 'Longitude']].copy()

    # Standardize the features
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)

    # Apply K-Means clustering
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(scaled_features)

    # Store cluster labels in the DataFrame
    clustered_data[f'{pollutant}_Cluster'] = clusters

# Print the resulting DataFrame with cluster labels
print(clustered_data.head())

# Assuming clustered_data contains cluster labels for each pollutant
# Initialize the DataFrame to store potential sources
potential_sources = pd.DataFrame(index=merged_data.index)

# Analyze clusters and identify potential sources
for pollutant in pollutants:
    cluster_col = f'{pollutant}_Cluster'
    cluster_labels = clustered_data[cluster_col].unique()

    for cluster in cluster_labels:
        cluster_data = merged_data[clustered_data[cluster_col] == cluster]

        # Analyze potential sources within this cluster
        # For example, calculate the mean location of sensors in the cluster
        mean_lat = cluster_data['Latitude'].mean()
        mean_lon = cluster_data['Longitude'].mean()

        # Store potential source information
        potential_sources.loc[clustered_data[cluster_col] == cluster, f'{pollutant}_Potential_Source_Lat'] = mean_lat
        potential_sources.loc[clustered_data[cluster_col] == cluster, f'{pollutant}_Potential_Source_Lon'] = mean_lon

# Print the potential sources DataFrame
print(potential_sources.head())

# Feature Selection and Data Preprocessing
features = merged_data.drop(columns=['Timestamp', 'Sensor_ID', 'Latitude', 'Longitude'])
labels = merged_data[['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

# Feature selection using RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
mse = mean_squared_error(y_test, model.predict(X_test))
print("Mean Squared Error:", mse)

# Cross-validation
cv_scores = cross_val_score(model, X_train, y_train, cv=4)
print("Cross-validation scores:", cv_scores)

# Fit RandomForestRegressor to the data
model.fit(features, labels)

# Plot feature importances
feature_importances = model.feature_importances_
feature_names = features.columns
plt.figure(figsize=(12, 6))
plt.barh(feature_names, feature_importances)
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.show()

# Assuming merged_data contains pollution data
# Perform clustering and identify potential pollution sources
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(merged_data[['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level']])
merged_data['Cluster'] = clusters

# Plot clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(data=merged_data, x='Longitude', y='Latitude', hue='Cluster', palette='viridis')
plt.title('Clusters of Pollution Levels')
plt.show()

# Select the pollutants and geographical coordinates for clustering
clustering_features = merged_data[['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level', 'Latitude', 'Longitude']]

# Apply PCA to reduce dimensionality
pca = PCA(n_components=2)
clustering_features_pca = pca.fit_transform(clustering_features)

# KMeans clustering
kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(clustering_features_pca)

# Add cluster labels to the original data
merged_data['Cluster'] = clusters

# Visualize the clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(x=clustering_features_pca[:, 0], y=clustering_features_pca[:, 1], hue=clusters, palette='viridis')
plt.title('Clusters of Pollution Levels (PCA-reduced)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

# Model Training and Evaluation
# Select the features and labels for prediction
X = merged_data.drop(columns=['Timestamp','NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level', 'Cluster']) # Dropping the 'Timestamp' column
y = merged_data[['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level']]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a RandomForestRegressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Plot feature importances
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]
plt.figure(figsize=(12, 6))
plt.title('Feature Importances')
plt.bar(range(X.shape[1]), importances[indices], align='center')
plt.xticks(range(X.shape[1]), [X.columns[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

# Function to perform Kriging interpolation for a pollutant
def perform_kriging(data, pollutant):
    # Define the grid for interpolation
    grid_lon = np.linspace(data['Longitude'].min(), data['Longitude'].max(), 50)
    grid_lat = np.linspace(data['Latitude'].min(), data['Latitude'].max(), 50)
    grid_lon, grid_lat = np.meshgrid(grid_lon, grid_lat)

    # Perform Ordinary Kriging
    OK = OrdinaryKriging(data['Longitude'], data['Latitude'], data[pollutant],
                         variogram_model='linear',
                         verbose=False,
                         enable_plotting=False)
    z, ss = OK.execute('grid', grid_lon, grid_lat)

    # Convert z to regular numpy array (if it's a MaskedArray)
    z = np.array(z)

    return grid_lon, grid_lat, z

# # Visualization of results
# # Create scatter plots to visualize the relationship between predicted and actual values
# pollutants = ['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level']

# for i, pollutant in enumerate(pollutants):
#     plt.figure(figsize=(8, 6))
#     sns.scatterplot(x=y_test.iloc[:, i], y=y_pred[:, i])
#     plt.xlabel('Actual Values')
#     plt.ylabel('Predicted Values')
#     plt.title(f'Actual vs Predicted Values for {pollutant}')
#     plt.show()

# # Read sensor data from CSV
# file_path = 'sensor_data.csv'  # Replace with your actual CSV file path
# data = pd.read_csv(file_path)

# # List of pollutants to interpolate
# pollutants = ['NO2_Level', 'PM25_Level', 'PM10_Level', 'O3_Level', 'NH3_Level', 'SO2_Level', 'CO_Level']

# # Create interactive maps for each pollutant
# for pollutant in pollutants:
#     create_interactive_map(data, pollutant)

# Function to create an interactive map showing the interpolated results
def create_interactive_map(data, pollutant):
    grid_lon, grid_lat, z = perform_kriging(data, pollutant)

    # Initialize the map centered around the mean of coordinates
    center_lat = np.mean(data['Latitude'])
    center_lon = np.mean(data['Longitude'])
    map_osm = folium.Map(location=[center_lat, center_lon], zoom_start=10)

    # Add the interpolated values as a heatmap layer
    heatmap_layer = folium.raster_layers.ImageOverlay(
        z,
        bounds=[[grid_lat.min(), grid_lon.min()], [grid_lat.max(), grid_lon.max()]],
        opacity=0.05,  # More transparent
        origin='lower',
        colormap=lambda x: (0, 0, x, x),
        name=f'{pollutant} concentration'
    ).add_to(map_osm)

    # Add interpolated value markers with tooltips
    for lat, lon, value in zip(grid_lat.flatten(), grid_lon.flatten(), z.flatten()):
        folium.CircleMarker(
            [lat, lon],
            radius=5,
            popup=f"{pollutant}: {value:.2f}",
            color='white',  # Adjust color as needed
            fill=True,
            fill_color='white',  # Adjust color as needed
            fill_opacity=0.01,  # More transparent
            tooltip=f"Interpolated {pollutant}: {value:.2f}"
        ).add_to(map_osm)

    # Add the sensor locations as markers
    for idx, row in data.iterrows():
        folium.Marker(
            [row['Latitude'], row['Longitude']],
            popup=f"Sensor {pollutant}: {row[pollutant]}",
            icon=folium.Icon(color='red', icon='info-sign')  # Adjust color and icon as needed
        ).add_to(map_osm)

    # Add layer control to toggle heatmap and markers
    folium.LayerControl().add_to(map_osm)

    # Display map in Jupyter notebook
    display(map_osm)
# Create interactive maps for each pollutant
for pollutant in pollutants:
    create_interactive_map(data, pollutant)